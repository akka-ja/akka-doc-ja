# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2011-2016, Lightbend Inc
# This file is distributed under the same license as the Akka package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Akka @version@\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-10-03 22:29+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.3.4\n"

#: ../../scala/stream/stream-cookbook.rst:5
msgid "Streams Cookbook"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:8
msgid "Introduction"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:10
msgid ""
"This is a collection of patterns to demonstrate various usage of the Akka"
" Streams API by solving small targeted problems in the format of "
"\"recipes\". The purpose of this page is to give inspiration and ideas "
"how to approach various small tasks involving streams. The recipes in "
"this page can be used directly as-is, but they are most powerful as "
"starting points: customization of the code snippets is warmly encouraged."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:15
msgid ""
"This part also serves as supplementary material for the main body of "
"documentation. It is a good idea to have this page open while reading the"
" manual and look for examples demonstrating various streaming concepts as"
" they appear in the main body of documentation."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:19
msgid ""
"If you need a quick reference of the available processing stages used in "
"the recipes see :ref:`stages-overview_scala`."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:22
msgid "Working with Flows"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:24
msgid ""
"In this collection we show simple recipes that involve linear flows. The "
"recipes in this section are rather general, more targeted recipes are "
"available as separate sections (:ref:`stream-rate-scala`, :ref:`stream-"
"io-scala`)."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:28
msgid "Logging elements of a stream"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:30
msgid ""
"**Situation:** During development it is sometimes helpful to see what "
"happens in a particular section of a stream."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:32
msgid ""
"The simplest solution is to simply use a ``map`` operation and use "
"``println`` to print the elements received to the console. While this "
"recipe is rather simplistic, it is often suitable for a quick debug "
"session."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:37
msgid ""
"Another approach to logging is to use ``log()`` operation which allows "
"configuring logging for elements flowing through the stream as well as "
"completion and erroring."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:43
msgid "Flattening a stream of sequences"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:45
msgid ""
"**Situation:** A stream is given as a stream of sequence of elements, but"
" a stream of elements needed instead, streaming all the nested elements "
"inside the sequences separately."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:48
msgid ""
"The ``mapConcat`` operation can be used to implement a one-to-many "
"transformation of elements using a mapper function in the form of ``In =>"
" immutable.Seq[Out]``. In this case we want to map a ``Seq`` of elements "
"to the elements in the collection itself, so we can just call "
"``mapConcat(identity)``."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:55
msgid "Draining a stream to a strict collection"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:57
msgid ""
"**Situation:** A possibly unbounded sequence of elements is given as a "
"stream, which needs to be collected into a Scala collection while "
"ensuring boundedness"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:59
msgid ""
"A common situation when working with streams is one where we need to "
"collect incoming elements into a Scala collection. This operation is "
"supported via ``Sink.seq`` which materializes into a ``Future[Seq[T]]``."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:62
msgid ""
"The function ``limit`` or ``take`` should always be used in conjunction "
"in order to guarantee stream boundedness, thus preventing the program "
"from running out of memory."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:64
msgid "For example, this is best avoided:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:68
msgid ""
"Rather, use ``limit`` or ``take`` to ensure that the resulting ``Seq`` "
"will contain only up to ``max`` elements:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:73
msgid "Calculating the digest of a ByteString stream"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:75
msgid ""
"**Situation:** A stream of bytes is given as a stream of ``ByteStrings`` "
"and we want to calculate the cryptographic digest of the stream."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:78
msgid ""
"This recipe uses a :class:`GraphStage` to host a mutable "
":class:`MessageDigest` class (part of the Java Cryptography API) and "
"update it with the bytes arriving from the stream. When the stream "
"starts, the ``onPull`` handler of the stage is called, which just bubbles"
" up the ``pull`` event to its upstream. As a response to this pull, a "
"ByteString chunk will arrive (``onPush``) which we use to update the "
"digest, then it will pull for the next chunk."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:83
msgid ""
"Eventually the stream of ``ByteStrings`` depletes and we get a "
"notification about this event via ``onUpstreamFinish``. At this point we "
"want to emit the digest value, but we cannot do it with ``push`` in this "
"handler directly since there may be no downstream demand. Instead we call"
" ``emit`` which will temporarily replace the handlers, emit the provided "
"value when demand comes in and then reset the stage state. It will then "
"complete the stage."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:93
msgid "Parsing lines from a stream of ByteStrings"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:95
msgid ""
"**Situation:** A stream of bytes is given as a stream of ``ByteStrings`` "
"containing lines terminated by line ending characters (or, alternatively,"
" containing binary frames delimited by a special delimiter byte sequence)"
" which needs to be parsed."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:99
msgid ""
"The :class:`Framing` helper object contains a convenience method to parse"
" messages from a stream of ``ByteStrings``:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:104
msgid "Implementing reduce-by-key"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:106
msgid ""
"**Situation:** Given a stream of elements, we want to calculate some "
"aggregated value on different subgroups of the elements."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:109
msgid ""
"The \"hello world\" of reduce-by-key style operations is *wordcount* "
"which we demonstrate below. Given a stream of words we first create a new"
" stream that groups the words according to the ``identity`` function, "
"i.e. now we have a stream of streams, where every substream will serve "
"identical words."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:113
msgid ""
"To count the words, we need to process the stream of streams (the actual "
"groups containing identical words). ``groupBy`` returns a "
":class:`SubFlow`, which means that we transform the resulting substreams "
"directly. In this case we use the ``reduce`` combinator to aggregate the "
"word itself and the number of its occurrences within a tuple "
":class:`(String, Integer)`. Each substream will then emit one final "
"value—precisely such a pair—when the overall input completes. As a last "
"step we merge back these values from the substreams into one single "
"output stream."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:122
msgid ""
"One noteworthy detail pertains to the ``MaximumDistinctWords`` parameter:"
" this defines the breadth of the groupBy and merge operations. Akka "
"Streams is focused on bounded resource consumption and the number of "
"concurrently open inputs to the merge operator describes the amount of "
"resources needed by the merge itself.  Therefore only a finite number of "
"substreams can be active at any given time. If the ``groupBy`` operator "
"encounters more keys than this number then the stream cannot continue "
"without violating its resource bound, in this case ``groupBy`` will "
"terminate with a failure."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:133
msgid "By extracting the parts specific to *wordcount* into"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:135
msgid "a ``groupKey`` function that defines the groups"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:136
msgid ""
"a ``map`` map each element to value that is used by the reduce on the "
"substream"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:137
msgid "a ``reduce`` function that does the actual reduction"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:139
msgid "we get a generalized version below:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:144
msgid ""
"Please note that the reduce-by-key version we discussed above is "
"sequential in reading the overall input stream, in other words it is "
"**NOT** a parallelization pattern like MapReduce and similar frameworks."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:149
msgid "Sorting elements to multiple groups with groupBy"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:151
msgid ""
"**Situation:** The ``groupBy`` operation strictly partitions incoming "
"elements, each element belongs to exactly one group. Sometimes we want to"
" map elements into multiple groups simultaneously."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:154
msgid "To achieve the desired result, we attack the problem in two steps:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:156
msgid ""
"first, using a function ``topicMapper`` that gives a list of topics "
"(groups) a message belongs to, we transform our stream of ``Message`` to "
"a stream of ``(Message, Topic)`` where for each topic the message belongs"
" to a separate pair will be emitted. This is achieved by using "
"``mapConcat``"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:159
msgid ""
"Then we take this new stream of message topic pairs (containing a "
"separate pair for each topic a given message belongs to) and feed it into"
" groupBy, using the topic as the group key."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:165
msgid "Working with Graphs"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:167
msgid ""
"In this collection we show recipes that use stream graph elements to "
"achieve various goals."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:170
msgid "Triggering the flow of elements programmatically"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:172
msgid ""
"**Situation:** Given a stream of elements we want to control the emission"
" of those elements according to a trigger signal. In other words, even if"
" the stream would be able to flow (not being backpressured) we want to "
"hold back elements until a trigger signal arrives."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:176
msgid ""
"This recipe solves the problem by simply zipping the stream of "
"``Message`` elements with the stream of ``Trigger`` signals. Since "
"``Zip`` produces pairs, we simply map the output stream selecting the "
"first element of the pair."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:181
msgid ""
"Alternatively, instead of using a ``Zip``, and then using ``map`` to get "
"the first element of the pairs, we can avoid creating the pairs in the "
"first place by using ``ZipWith`` which takes a two argument function to "
"produce the output element. If this function would return a pair of the "
"two argument it would be exactly the behavior of ``Zip`` so ``ZipWith`` "
"is a generalization of zipping."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:191
msgid "Balancing jobs to a fixed pool of workers"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:193
msgid ""
"**Situation:** Given a stream of jobs and a worker process expressed as a"
" :class:`Flow` create a pool of workers that automatically balances "
"incoming jobs to available workers, then merges the results."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:196
msgid ""
"We will express our solution as a function that takes a worker flow and "
"the number of workers to be allocated and gives a flow that internally "
"contains a pool of these workers. To achieve the desired result we will "
"create a :class:`Flow` from a graph."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:200
msgid ""
"The graph consists of a ``Balance`` node which is a special fan-out "
"operation that tries to route elements to available downstream consumers."
" In a ``for`` loop we wire all of our desired workers as outputs of this "
"balancer element, then we wire the outputs of these workers to a "
"``Merge`` element that will collect the results from the workers."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:204
msgid ""
"To make the worker stages run in parallel we mark them as asynchronous "
"with `async`."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:209
msgid "Working with rate"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:211
msgid ""
"This collection of recipes demonstrate various patterns where rate "
"differences between upstream and downstream needs to be handled by other "
"strategies than simple backpressure."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:215
msgid "Dropping elements"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:217
msgid ""
"**Situation:** Given a fast producer and a slow consumer, we want to drop"
" elements if necessary to not slow down the producer too much."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:220
msgid ""
"This can be solved by using a versatile rate-transforming operation, "
"``conflate``. Conflate can be thought as a special ``reduce`` operation "
"that collapses multiple upstream elements into one aggregate element if "
"needed to keep the speed of the upstream unaffected by the downstream."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:224
msgid ""
"When the upstream is faster, the reducing process of the ``conflate`` "
"starts. Our reducer function simply takes the freshest element. This in a"
" simple dropping operation."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:229
msgid ""
"There is a more general version of ``conflate`` named "
"``conflateWithSeed`` that allows to express more complex aggregations, "
"more similar to a ``fold``."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:233
msgid "Dropping broadcast"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:235
msgid ""
"**Situation:** The default ``Broadcast`` graph element is properly "
"backpressured, but that means that a slow downstream consumer can hold "
"back the other downstream consumers resulting in lowered throughput. In "
"other words the rate of ``Broadcast`` is the rate of its slowest "
"downstream consumer. In certain cases it is desirable to allow faster "
"consumers to progress independently of their slower siblings by dropping "
"elements if necessary."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:240
msgid ""
"One solution to this problem is to append a ``buffer`` element in front "
"of all of the downstream consumers defining a dropping strategy instead "
"of the default ``Backpressure``. This allows small temporary rate "
"differences between the different consumers (the buffer smooths out small"
" rate variances), but also allows faster consumers to progress by "
"dropping from the buffer of the slow consumers if necessary."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:248
msgid "Collecting missed ticks"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:250
msgid ""
"**Situation:** Given a regular (stream) source of ticks, instead of "
"trying to backpressure the producer of the ticks we want to keep a "
"counter of the missed ticks instead and pass it down when possible."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:253
msgid ""
"We will use ``conflateWithSeed`` to solve the problem. The seed version "
"of conflate takes two functions:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:255
msgid ""
"A seed function that produces the zero element for the folding process "
"that happens when the upstream is faster than the downstream. In our case"
" the seed function is a constant function that returns 0 since there were"
" no missed ticks at that point."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:258
msgid ""
"A fold function that is invoked when multiple upstream messages needs to "
"be collapsed to an aggregate value due to the insufficient processing "
"rate of the downstream. Our folding function simply increments the "
"currently stored count of the missed ticks so far."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:262
msgid ""
"As a result, we have a flow of ``Int`` where the number represents the "
"missed ticks. A number 0 means that we were able to consume the tick fast"
" enough (i.e. zero means: 1 non-missed tick + 0 missed ticks)"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:268
msgid "Create a stream processor that repeats the last element seen"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:270
msgid ""
"**Situation:** Given a producer and consumer, where the rate of neither "
"is known in advance, we want to ensure that none of them is slowing down "
"the other by dropping earlier unconsumed elements from the upstream if "
"necessary, and repeating the last value for the downstream if necessary."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:274
msgid ""
"We have two options to implement this feature. In both cases we will use "
":class:`GraphStage` to build our custom element. In the first version we "
"will use a provided initial value ``initial`` that will be used to feed "
"the downstream if no upstream element is ready yet. In the ``onPush()`` "
"handler we just overwrite the ``currentValue`` variable and immediately "
"relieve the upstream by calling ``pull()``. The downstream ``onPull`` "
"handler is very similar, we immediately relieve the downstream by "
"emitting ``currentValue``."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:282
msgid ""
"While it is relatively simple, the drawback of the first version is that "
"it needs an arbitrary initial element which is not always possible to "
"provide. Hence, we create a second version where the downstream might "
"need to wait in one single case: if the very first element is not yet "
"available."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:286
msgid ""
"We introduce a boolean variable ``waitingFirstValue`` to denote whether "
"the first element has been provided or not (alternatively an "
":class:`Option` can be used for ``currentValue`` or if the element type "
"is a subclass of AnyRef a null can be used with the same purpose). In the"
" downstream ``onPull()`` handler the difference from the previous version"
" is that we check if we have received the first value and only emit if we"
" have. This leads to that when the first element comes in we must check "
"if there possibly already was demand from downstream so that we in that "
"case can push the element directly."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:296
msgid "Globally limiting the rate of a set of streams"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:298
msgid ""
"**Situation:** Given a set of independent streams that we cannot merge, "
"we want to globally limit the aggregate throughput of the set of streams."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:301
msgid ""
"One possible solution uses a shared actor as the global limiter combined "
"with mapAsync to create a reusable :class:`Flow` that can be plugged into"
" a stream to limit its rate."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:304
msgid ""
"As the first step we define an actor that will do the accounting for the "
"global rate limit. The actor maintains a timer, a counter for pending "
"permit tokens and a queue for possibly waiting participants. The actor "
"has an ``open`` and ``closed`` state. The actor is in the ``open`` state "
"while it has still pending permits. Whenever a request for permit arrives"
" as a ``WantToPass`` message to the actor the number of available permits"
" is decremented and we notify the sender that it can pass by answering "
"with a ``MayPass`` message. If the amount of permits reaches zero, the "
"actor transitions to the ``closed`` state. In this state requests are not"
" immediately answered, instead the reference of the sender is added to a "
"queue. Once the timer for replenishing the pending permits fires by "
"sending a ``ReplenishTokens`` message, we increment the pending permits "
"counter and send a reply to each of the waiting senders. If there are "
"more waiting senders than permits available we will stay in the "
"``closed`` state."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:316
msgid ""
"To create a Flow that uses this global limiter actor we use the "
"``mapAsync`` function with the combination of the ``ask`` pattern. We "
"also define a timeout, so if a reply is not received during the "
"configured maximum wait period the returned future from ``ask`` will "
"fail, which will fail the corresponding stream as well."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:323
msgid ""
"The global actor used for limiting introduces a global bottleneck. You "
"might want to assign a dedicated dispatcher for this actor."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:327
msgid "Working with IO"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:330
msgid "Chunking up a stream of ByteStrings into limited size ByteStrings"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:332
msgid ""
"**Situation:** Given a stream of ByteStrings we want to produce a stream "
"of ByteStrings containing the same bytes in the same sequence, but "
"capping the size of ByteStrings. In other words we want to slice up "
"ByteStrings into smaller chunks if they exceed a size threshold."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:336
msgid ""
"This can be achieved with a single :class:`GraphStage`. The main logic of"
" our stage is in ``emitChunk()`` which implements the following logic:"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:339
msgid ""
"if the buffer is empty, and upstream is not closed we pull for more "
"bytes, if it is closed we complete"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:340
msgid ""
"if the buffer is nonEmpty, we split it according to the ``chunkSize``. "
"This will give a next chunk that we will emit, and an empty or nonempty "
"remaining buffer."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:343
msgid ""
"Both ``onPush()`` and ``onPull()`` calls ``emitChunk()`` the only "
"difference is that the push handler also stores the incoming chunk by "
"appending to the end of the buffer."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:349
msgid "Limit the number of bytes passing through a stream of ByteStrings"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:351
msgid ""
"**Situation:** Given a stream of ByteStrings we want to fail the stream "
"if more than a given maximum of bytes has been consumed."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:354
msgid ""
"This recipe uses a :class:`GraphStage` to implement the desired feature. "
"In the only handler we override, ``onPush()`` we just update a counter "
"and see if it gets larger than ``maximumBytes``. If a violation happens "
"we signal failure, otherwise we forward the chunk we have received."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:361
msgid "Compact ByteStrings in a stream of ByteStrings"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:363
msgid ""
"**Situation:** After a long stream of transformations, due to their "
"immutable, structural sharing nature ByteStrings may refer to multiple "
"original ByteString instances unnecessarily retaining memory. As the "
"final step of a transformation chain we want to have clean copies that "
"are no longer referencing the original ByteStrings."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:367
msgid ""
"The recipe is a simple use of map, calling the ``compact()`` method of "
"the :class:`ByteString` elements. This does copying of the underlying "
"arrays, so this should be the last element of a long chain if used."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:373
msgid "Injecting keep-alive messages into a stream of ByteStrings"
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:375
msgid ""
"**Situation:** Given a communication channel expressed as a stream of "
"ByteStrings we want to inject keep-alive messages but only if this does "
"not interfere with normal traffic."
msgstr ""

#: ../../scala/stream/stream-cookbook.rst:378
msgid "There is a built-in operation that allows to do this directly:"
msgstr ""

